{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hiperparámetros de árboles de decisión: **de la teoría a su implementación en scikit-learn**\n",
        "\n",
        "Los árboles de decisión controlan su complejidad mediante un conjunto de hiperparámetros que regulan **qué tan fino** es el particionado del espacio de características y **cuándo** dejar de crecer o **cómo** podar. A continuación se presentan estos controles **en términos teóricos (siguiendo la notación de *Introduction to Statistical Learning*)**, y luego su **mapeo directo** a `DecisionTreeClassifier` de *scikit-learn*, con notas prácticas sobre su efecto en **sesgo–varianza**, estabilidad y eficiencia.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Criterio de partición (función de impureza / ganancia de información)\n",
        "\n",
        "**Teoría.**  \n",
        "Sea $p_{mk}$ la proporción de observaciones en el nodo $m$ que pertenecen a la clase $k$ (con $k=1,\\dots,K$). La impureza mide cuán “mezclado” está un nodo.\n",
        "\n",
        "- **Índice de Gini**:\n",
        "$$\n",
        "G(m) = \\sum_{k=1}^K p_{mk}(1 - p_{mk}) \\;=\\; 1 - \\sum_{k=1}^K p_{mk}^2\n",
        "$$\n",
        "\n",
        "- **Entropía**:\n",
        "$$\n",
        "H(m) = - \\sum_{k=1}^K p_{mk}\\,\\log(p_{mk})\n",
        "$$\n",
        "\n",
        "La reducción en impureza al dividir un nodo $m$ en hijos izquierdo ($L$) y derecho ($R$) se calcula como:\n",
        "$$\n",
        "\\Delta = \\mathrm{Impureza}(m) \\;-\\; \\frac{N_L}{N_m}\\,\\mathrm{Impureza}(L) \\;-\\; \\frac{N_R}{N_m}\\,\\mathrm{Impureza}(R),\n",
        "$$\n",
        "donde $N_m$ es el número de observaciones en $m$, y $N_L$, $N_R$ corresponden a los nodos hijos.\n",
        "\n",
        "**En scikit-learn.** `criterion={\"gini\",\"entropy\",\"log_loss\"}`\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Estrategia de búsqueda del split\n",
        "\n",
        "**Teoría.** Al evaluar divisiones, se puede elegir de forma determinista la mejor según el criterio, o introducir aleatoriedad para generar árboles distintos (útil en ensambles).\n",
        "\n",
        "**En scikit-learn.** `splitter={\"best\",\"random\"}`  \n",
        "- `best`: busca exhaustivamente el mejor split.  \n",
        "- `random`: selecciona aleatoriamente un subconjunto de splits candidatos.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Criterios de **pre-poda** (detener el crecimiento antes de tiempo)\n",
        "\n",
        "**Teoría.** El crecimiento de un árbol sin restricciones tiende a generar sobreajuste: baja varianza en entrenamiento pero mala generalización. Para evitarlo, se fijan umbrales de complejidad:\n",
        "\n",
        "- **Profundidad máxima ($\\max\\_depth$).** Limita el número de niveles. Árboles más profundos capturan más detalle, pero arriesgan sobreajuste.  \n",
        "- **Número mínimo de observaciones para dividir ($\\min\\_samples\\_split$).** Un nodo sólo se divide si contiene al menos ese número de observaciones.  \n",
        "- **Número mínimo de observaciones por hoja ($\\min\\_samples\\_leaf$).** Obliga a que cada hoja contenga una fracción significativa de observaciones.  \n",
        "- **Mínima reducción de impureza ($\\min\\_impurity\\_decrease$).** Un split sólo se realiza si reduce la impureza al menos en esa cantidad.  \n",
        "- **Número máximo de hojas ($\\max\\_leaf\\_nodes$).** Limita directamente la cantidad de nodos terminales.  \n",
        "- **Fracción mínima de peso en hojas ($\\min\\_weight\\_fraction\\_leaf$).** Variante de `min_samples_leaf` cuando se usan pesos.\n",
        "\n",
        "**En scikit-learn.**\n",
        "```python\n",
        "DecisionTreeClassifier(\n",
        "  max_depth=6,\n",
        "  min_samples_split=10,\n",
        "  min_samples_leaf=5,\n",
        "  min_impurity_decrease=1e-3,\n",
        "  max_leaf_nodes=None,\n",
        "  min_weight_fraction_leaf=0.0,\n",
        "  random_state=42\n",
        ")\n",
        "```\n",
        "Estos parámetros permiten reducir la varianza y mejoran la generalización.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Submuestreo de variables en cada split\n",
        "\n",
        "**Teoría.** En cada nodo puede evaluarse un subconjunto de variables en lugar de todas. Esto introduce variabilidad y ayuda a reducir la dependencia de una única característica dominante. En ensambles como Random Forests, este mecanismo es esencial para reducir la correlación entre árboles.\n",
        "\n",
        "**En scikit-learn.**  \n",
        "Parámetro: `max_features: int | float | {\"sqrt\",\"log2\"} | None`  \n",
        "\n",
        "- `None`: se usan todas las variables.  \n",
        "- `sqrt`: usa la raíz cuadrada del número total de variables.  \n",
        "- `log2`: usa el logaritmo base 2 del número total de variables.  \n",
        "- `int` o `float`: define cuántas variables (o fracción) se consideran en cada split.\n",
        "\n",
        "**Efecto.**  \n",
        "- Con todas las variables, el árbol es más flexible, pero puede sobreajustar.  \n",
        "- Al limitar `max_features`, se introduce más sesgo pero menos varianza.  \n",
        "- Es muy útil cuando $p \\gg n$ (muchas variables respecto al número de observaciones) o cuando existen variables irrelevantes.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) **Post-poda** por costo-complejidad\n",
        "\n",
        "**Teoría.** Una vez crecido el árbol, se aplica la poda basada en costo-complejidad:\n",
        "$$\n",
        "R_\\alpha(T) = R(T) + \\alpha\\,|T|,\n",
        "$$\n",
        "donde $R(T)$ es el error de entrenamiento del árbol $T$, $|T|$ es el número de hojas, y $\\alpha \\geq 0$ es un parámetro de penalización. A mayor $\\alpha$, más se favorecen árboles pequeños.\n",
        "\n",
        "**En scikit-learn.**  \n",
        "Parámetro: `ccp_alpha: float ≥ 0`  \n",
        "\n",
        "- `0.0`: sin poda.  \n",
        "- Valores positivos: se eliminan ramas con poca contribución.\n",
        "\n",
        "**Práctica recomendada.**  \n",
        "Utilizar el método `cost_complexity_pruning_path` para explorar el camino de poda y seleccionar $\\alpha$ mediante validación cruzada.\n",
        "(ver https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)\n",
        "\n",
        "**Efecto práctico.**  \n",
        "- Disminuye la varianza y mejora interpretabilidad.  \n",
        "- La elección adecuada de $\\alpha$ permite mantener un buen desempeño en test, evitando árboles demasiado complejos.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Ponderación y clases desbalanceadas\n",
        "\n",
        "**Teoría.** En problemas desbalanceados, los errores de la clase minoritaria deben recibir más peso. Esto evita que el árbol favorezca desproporcionadamente a la clase mayoritaria.\n",
        "\n",
        "**En scikit-learn.**  \n",
        "- `class_weight={\"balanced\", dict}`: ajusta automáticamente los pesos en función de la frecuencia de clases o permite definirlos manualmente.  \n",
        "- `sample_weight` en `fit(...)`: pondera instancias específicas.\n",
        "\n",
        "**Efecto.**  \n",
        "- Cambia los cálculos de impureza y, por ende, los umbrales óptimos de split.  \n",
        "- Mejora el recall de clases minoritarias, aunque puede reducir la precisión en la clase mayoritaria.\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Naturaleza de las variables y datos faltantes\n",
        "\n",
        "**Teoría.**  \n",
        "- Variables continuas: los árboles las dividen mediante umbrales.  \n",
        "- Variables categóricas: en teoría, un árbol puede dividir según subconjuntos de categorías.  \n",
        "- Datos faltantes: algunos algoritmos incorporan *surrogate splits* para manejar ausencias.\n",
        "\n",
        "**En scikit-learn.**  \n",
        "- No soporta categóricas puras de forma nativa → se requiere codificación previa (one-hot, ordinal, etc.).  \n",
        "- No maneja `NaN` directamente → se recomienda imputación (ej. `SimpleImputer`).\n",
        "\n",
        "**Buenas prácticas.**  \n",
        "Encadenar preprocesamiento y modelo en un `Pipeline` para evitar *data leakage* durante validación cruzada.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Aleatoriedad y reproducibilidad\n",
        "\n",
        "**Teoría.** Si existen empates en los splits, el algoritmo puede dar árboles distintos en ejecuciones diferentes. Para garantizar resultados reproducibles, se fija una semilla de aleatoriedad.\n",
        "\n",
        "**En scikit-learn.**  \n",
        "Parámetro: `random_state`\n",
        "\n",
        "**Efecto.**  \n",
        "- Permite reproducibilidad en experimentos.  \n",
        "- Fundamental al comparar configuraciones o al enseñar.\n",
        "\n",
        "---\n",
        "\n",
        "## 9) Métrica de evaluación y protocolo de validación\n",
        "\n",
        "**Teoría.** El ajuste de hiperparámetros depende de la métrica y protocolo usados para evaluar el modelo. Accuracy es común, pero en problemas desbalanceados conviene F1 o ROC-AUC. La validación cruzada estratificada es preferida para árboles de clasificación.\n",
        "\n",
        "**En scikit-learn.**  \n",
        "- Parámetro `scoring` en GridSearchCV/RandomizedSearchCV (`\"accuracy\"`, `\"f1\"`, `\"roc_auc\"`, etc.).  \n",
        "- Esquemas de CV como `StratifiedKFold`.\n",
        "\n",
        "**Efecto.**  \n",
        "La elección de métrica determina qué configuraciones se consideran “óptimas”. En datasets desbalanceados, usar sólo accuracy puede inducir a modelos engañosamente buenos.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Itinerarios de *tuning* y recomendaciones\n",
        "\n",
        "1. **Comenzar con pre-poda simple**: fijar `max_depth` y `min_samples_leaf` para evitar sobreajuste evidente.  \n",
        "2. **Refinar** con `min_samples_split` y, si hay muchas variables, con `max_features`.  \n",
        "3. **Aplicar post-poda** (`ccp_alpha`) validada por CV para simplificar el modelo sin sacrificar rendimiento.  \n",
        "4. **Seleccionar la métrica** adecuada al problema (ej. F1 o ROC-AUC en desbalance).  \n",
        "5. **Validar cuidadosamente** con esquemas estratificados y, cuando sea necesario, CV anidada.  \n",
        "6. Para espacios grandes de búsqueda, preferir **Optuna** u optimizadores bayesianos en lugar de GridSearch.\n",
        "\n",
        "---\n",
        "\n",
        "## Resumen\n",
        "\n",
        "Los hiperparámetros de los árboles de decisión corresponden a mecanismos de **control de complejidad** (pre-poda, submuestreo) y de **regularización explícita** (post-poda). Su elección debe ser **consciente**, en función del balance sesgo–varianza y de la métrica de interés. Ajustarlos correctamente permite obtener modelos que generalicen bien y que sean interpretables.\n",
        "\n"
      ],
      "metadata": {
        "id": "syGhBXZVRMnT"
      }
    }
  ]
}