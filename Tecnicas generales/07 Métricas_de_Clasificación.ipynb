{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "@ Gustavo Vazquez\n",
        "\n",
        "# Métricas de Evaluación en Problemas de Clasificación\n",
        "\n",
        "Evaluar el desempeño de un modelo de clasificación es esencial para medir su efectividad y comprender sus fortalezas y limitaciones. Existen diversas métricas que proporcionan información específica sobre el rendimiento del clasificador, permitiendo una evaluación más detallada de sus predicciones.\n",
        "\n",
        "En el contexto de clasificación, el caso más común es la clasificación binaria, donde los datos se dividen en dos categorías distintas. Este tipo de problema es particularmente relevante cuando las clases representan conceptos opuestos. En estos casos, la correcta identificación de cada clase es crucial para seleccionar y aplicar las métricas de evaluación adecuadas.\n",
        "\n",
        "En este caso tendremos dos clases:\n",
        "\n",
        "- **Clase Positiva (P):** Es la clase de interés en el problema. Representa los casos que el modelo debe detectar con mayor precisión.\n",
        "- **Clase Negativa (N):** Es la clase complementaria. Son los casos que no pertenecen a la clase positiva.\n",
        "\n",
        "La elección de la clase positiva depende del contexto del problema. Algunos ejemplos:\n",
        "\n",
        "- En un modelo de detección de pacientes con patologías:\n",
        "  - **Positiva (P):** Paciente con patología.\n",
        "  - **Negativa (N):** Paciente sano.\n",
        "  - Un falso negativo (FN) es crítico, ya que podría llevar a no diagnosticar una enfermedad.\n",
        "\n",
        "- En un filtro de spam:\n",
        "  - **Positiva (P):** Correo clasificado como spam.\n",
        "  - **Negativa (N):** Correo legítimo.\n",
        "  - Un falso positivo (FP) significa que un correo válido se detectó erróneamente como spam.\n",
        "\n",
        "- En detección de fraudes bancarios:\n",
        "  - **Positiva (P):** Transacción fraudulenta.\n",
        "  - **Negativa (N):** Transacción legítima.\n",
        "  - Es preferible minimizar los falsos negativos (FN), ya que permitiría que una transacción fraudulenta pase desapercibida.\n",
        "\n",
        "Definir correctamente qué es la clase positiva y negativa afecta la interpretación de métricas como **precisión, recall y F1-score**.\n",
        "\n",
        "---\n",
        "## 2. Matriz de Confusión\n",
        "\n",
        "La **matriz de confusión** es una tabla que permite visualizar el desempeño del clasificador en términos de predicciones correctas e incorrectas. Está compuesta por cuatro valores clave:\n",
        "\n",
        "|                 | Clase Real Positiva (P) | Clase Real Negativa (N) |\n",
        "|----------------|------------------------|------------------------|\n",
        "| **Predicción Positiva** | Verdadero Positivo (TP) | Falso Positivo (FP) |\n",
        "| **Predicción Negativa** | Falso Negativo (FN) | Verdadero Negativo (TN) |\n",
        "\n",
        "- **Verdadero Positivo (TP):** Número de ejemplos correctamente clasificados como positivos.\n",
        "- **Falso Positivo (FP):** Número de ejemplos negativos clasificados erróneamente como positivos.\n",
        "- **Falso Negativo (FN):** Número de ejemplos positivos clasificados erróneamente como negativos.\n",
        "- **Verdadero Negativo (TN):** Número de ejemplos correctamente clasificados como negativos.\n",
        "\n",
        "La correcta interpretación de esta tabla es clave para definir las métricas que evaluarán el modelo.\n",
        "\n",
        "---\n",
        "## 3. Accuracy (Exactitud)\n",
        "\n",
        "**Accuracy** mide la proporción de predicciones correctas sobre el total de ejemplos. Se define como:\n",
        "\n",
        "$$\n",
        "Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\n",
        "$$\n",
        "\n",
        "### Ejemplo:\n",
        "Supongamos que un modelo clasifica si un correo es **spam (positivo)** o **no spam (negativo)**. Evaluamos el modelo con 100 correos:\n",
        "- 80 clasificaciones correctas (50 spam y 30 no spam).\n",
        "- 10 correos spam mal clasificados como no spam.\n",
        "- 10 correos no spam mal clasificados como spam.\n",
        "\n",
        "Aplicamos la fórmula:\n",
        "\n",
        "$$\n",
        "Accuracy = \\frac{50 + 30}{50 + 10 + 30 + 10} = \\frac{80}{100} = 0.8 \\, (80\\%)\n",
        "$$\n",
        "\n",
        "### Limitaciones:\n",
        "- Si las clases están desbalanceadas, **accuracy puede ser engañosa**. Por ejemplo, si el 95% de los correos no son spam, un modelo que siempre predice \"no spam\" tendrá una accuracy del 95%, pero no será útil.\n",
        "\n",
        "---\n",
        "## 4. Precision (Precisión)\n",
        "\n",
        "La **precisión** mide cuántos de los ejemplos clasificados como positivos realmente lo son. Se define como:\n",
        "\n",
        "$$\n",
        "Precision = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "### Ejemplo:\n",
        "En el mismo problema de clasificación de spam:\n",
        "- El modelo predijo 60 correos como spam, pero 10 no eran realmente spam (**FP = 10**).\n",
        "- De los 60 predichos como spam, 50 sí lo eran (**TP = 50**).\n",
        "\n",
        "$$\n",
        "Precision = \\frac{50}{50 + 10} = \\frac{50}{60} = 0.8333 \\, (83.3\\%)\n",
        "$$\n",
        "\n",
        "---\n",
        "## 5. Recall (Sensibilidad o Exhaustividad)\n",
        "\n",
        "El **recall** mide cuántos de los ejemplos positivos fueron correctamente clasificados. Se define como:\n",
        "\n",
        "$$\n",
        "Recall = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "### Ejemplo:\n",
        "- En el problema del spam, el modelo clasificó correctamente 50 correos spam (**TP = 50**), pero falló en detectar 10 correos spam (**FN = 10**).\n",
        "\n",
        "$$\n",
        "Recall = \\frac{50}{50 + 10} = \\frac{50}{60} = 0.8333 \\, (83.3\\%)\n",
        "$$\n",
        "\n",
        "![Precision y Recall](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)\n",
        "\n",
        "---\n",
        "## 6. F1-Score\n",
        "\n",
        "El **F1-score** es la media armónica entre precisión y recall. Se usa cuando es necesario un balance entre ambas métricas.\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
        "$$\n",
        "\n",
        "### Ejemplo:\n",
        "- **Precision = 0.8333**, **Recall = 0.8333**\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{0.8333 \\times 0.8333}{0.8333 + 0.8333} = 0.8333\n",
        "$$\n",
        "\n",
        "---\n",
        "## 7. Ejemplo en Código: Evaluación de un Modelo de Clasificación"
      ],
      "metadata": {
        "id": "9tAnbQ42Mj5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generar un conjunto de datos más difícil de clasificar\n",
        "X, y = make_classification(n_samples=200, n_features=5, n_informative=3, n_redundant=2,\n",
        "                           class_sep=0.5, flip_y=0.15, random_state=42)\n",
        "\n",
        "# Dividir en entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar un modelo de clasificación (Regresión Logística) con iteraciones limitadas\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
        "\n",
        "# Calcular métricas de evaluación\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"Accuracy (Exactitud): {accuracy:.2f}\")\n",
        "print(f\"Precision (Precisión): {precision:.2f}\")\n",
        "print(f\"Recall (Exhaustividad): {recall:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnohPl_tRXL6",
        "outputId": "d9f40953-1a34-4ecd-fd00-832f858a0857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusión:\n",
            " [[17  3]\n",
            " [ 7 13]]\n",
            "Accuracy (Exactitud): 0.75\n",
            "Precision (Precisión): 0.81\n",
            "Recall (Exhaustividad): 0.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "## 8. Cómo Elegir una Métrica de Evaluación\n",
        "\n",
        "La elección depende de los objetivos:\n",
        "\n",
        "- Si ambas clases son igualmente importantes y el dataset está balanceado → **accuracy** es aceptable.  \n",
        "- Si lo importante es no perder casos positivos → **recall**.  \n",
        "- Si lo importante es minimizar falsos positivos → **precision**.  \n",
        "- Si se busca un equilibrio → **F1-score**.  \n",
        "- Si interesa la capacidad global de ranking → **ROC AUC**.  \n",
        "- Si interesa la calidad de las probabilidades → **Log-Loss**, **Brier Score**.  \n",
        "\n",
        "En problemas desbalanceados, es clave priorizar **precision**, **recall**, **F1-score** o métricas basadas en curvas (ROC, Precision-Recall), en lugar de depender únicamente de **accuracy**."
      ],
      "metadata": {
        "id": "I3q3Oe6Z7bGX"
      }
    }
  ]
}
